Contributor: Qianhan Zeng

> ä¸€äº›ç§‘ç ”å†™ä½œä¸­å¯ä»¥ä½¿ç”¨çš„å¥½è¯å¥½å¥å¥½ç»“æ„ï¼Œæ¥æºæ˜¯çœ‹åˆ°çš„æ–‡çŒ®æ•™æï¼ŒåšæŒåšå¤§è‡ªç„¶çš„æ¬è¿å·¥å’Œçµæ´»çš„å°è£ç¼ ğŸ¶
# Introduction

**ğŸš©ç”¨äºå½¢å®¹AIæŠ€æœ¯/æ—¶ä»£çš„èŠ±é‡Œèƒ¡å“¨çš„è¡¨è¾¾ï¼š**
- During the last decade, we have `witnessed` an `unprecedented explosion` of academic and popular interests in machine learning. 
- Deep learning has changed the `landscape` of image registration.
  

**ğŸš© å¸å¼•/æ¿€å‘/å¼ºè°ƒ+æ³¨æ„(attention)/å…´è¶£(interest)ï¼š**
- Crowdsourcing has `garnered` widespread interest.
- This has, in turn, `fueled` considerable interest in statistical modeling for location-referenced spatial data.
- The increased availability of labeled X-ray image archives (e.g. ChestX-ray14 dataset) has `triggered` a growing interest in deep learning techniques.
- As remarked in the previous section, we will `address` our attention to the second-order stationary processes. 
  
**ğŸš© ä»‹ç»æœ¬æ–‡å†…å®¹ï¼š**
- The objective of this work is `twofold`. First, XXX. Second, XXX.
- The framework `encompasses` both XXX and XXX.


# Literature

**ğŸš© æ€»èµ·ä»‹ç»å‰äººçš„å·¥ä½œï¼š**
- The limited scalability of interior point methods has `inspired a recent ï¬‚urry of work` on ï¬rst-order methods.
- `A line of recent research pursues` calculation of principal components under error minimization.
- As shown in Fig. 1, `a wealth of research is dedicated to` the applications of diffusion models in diverse medical imaging scenarios.
- Since diffusion models have recently received significant attention from the research community, the literature is experiencing `a large influx of` contributions in this direction.
- In recent years, a `flourishing line of research` addresses the very high-dimensional regime.

**ğŸš© æ‰¹è¯„å‰äººçš„å·¥ä½œï¼š**
- PCA is arguably the most widely used statistical tool for data analysis and dimensionality reduction today. However, its brittleness with respect to grossly corrupted observations often `puts its validity in jeopardy` â€“ a single grossly corrupted entry in M could render the estimated L arbitrarily far from the true L0.

**ğŸš© å¹æ§è‡ªå·±çš„å·¥ä½œï¼š**
- The data augmentation step makes the procedure `immune to overfitting`, so that the resulting prediction...
- This extension is substantial in both methodology and applications.
- Our approach `features` a one-stage continuous optimization-based implementation.

# Methodology

**ğŸš©satisfy a condition/assumption/structure/...**
- `admit` æ›¿æ¢ `satisfy`ï¼š*The original ordering may not `admit` a property/structure/condition.*
- `accomodate` è¿‘ä¼¼æ›¿æ¢ `satisfy`ï¼š*To `accommodate` a flexible correlation structure for our model, this article proposes a new multivariate logistic density.*
- `respect` æ›¿æ¢ `satisfy`ï¼š*All the above assumptions are `respected` by most of the popular kernels, in particular the Gaussian, Exponential, Uniform, Triangular, Cosine kernel, etc.*

**ğŸš© å…¨ç¯‡å›ºå®šä¸€ä¸ªå†™æ³•/å®šä¹‰/è¯´æ˜**
- Here and in the following we use the convention $0/0=0$.
- Throughout this paper, unless otherwise stated, $\|\cdot\|$ represents the $\ell_2$ norm of a vector and a matrix. We will use $C$ and $C_i$ to denote positive constants independent of $(n_k , K, N )$.
- Constants $c$, $C$, $c_1$, $\cdots$ are understood to be independent of $n$.
- From now on, we will always denote various positive absolute constants by $C$, $c$, $C_1$, $c_1$ without saying this explicitly.
- We use â€œconstâ€ to symbolize a constant that does not depend on the model parameter $\theta$ and hence does not affect optimization.
- Here and throughout, $T$, when used as a superscript, denotes matrix transposition.

**ğŸš© éœ€è¦æå‰ä½¿ç”¨ä¸€ä¸ªä¹‹åå†å®šä¹‰çš„ç¬¦å·**

- $\mu$ is the xxx parameter, `which will be formally defined later` in Assumption 1.
- For example, `as is shown below`, the function which minimizes the $L_2$ risk can be derived explicitly.
  

**ğŸš© éœ€è¦ç®€åŒ–ç¬¦å·**
- Here, $m_n(x) = m_n(x, D_n)$ is a measurable function of $x$ and the data. For simplicity, we will `suppress $D_n$ in the notation` and write $m_n(x)$ instead of $m_n(x, D_n)$.
- `For the ease of exposition`, we assume an equal sample size $m_i = m$.
- Let $g(u)$ and $f(u)$ be `a shorthand of` $g_\eta(u)$ and $f_\eta(u)$, respectively.


**ğŸš© ä¸¾å‡ºç›´è§‚çš„ä¾‹å­**
- In order to `build some intuition`
- This notion is best understood by `working through some illustrative examples`.
- As a `follow-up` to the previous example


**ğŸš© éœ€è¦èŠ‚çœç¯‡å¹…**
- `For space consideration`, we present XXX in the supplementary materials.

# Experiments

# Reference

- è‹±æ–‡æ ‡é¢˜è‡ªåŠ¨å¤§å°å†™ï¼šhttps://www.wordpressleaf.com/capitalize-title
  > ä¸€èˆ¬ä»è°·æ­Œå­¦æœ¯ä¸Šå¤åˆ¶bibtexï¼Œä½†æ˜¯å‚è€ƒæ–‡çŒ®çš„æ–‡ç« æ ‡é¢˜å¤§å°å†™å¾€å¾€é£æ ¼ä¸åŒï¼Œå¯èƒ½å…¨éƒ¨å¤§å†™ï¼Œå…¨éƒ¨å°å†™ï¼Œé¦–å­—æ¯å¤§å†™ç­‰ç­‰ï¼Œè¿™ä¸ªæ—¶å€™éœ€è¦æ‰‹åŠ¨è°ƒæ•´ï¼Œå¯ä»¥æŠŠæ ‡é¢˜å¤åˆ¶åˆ°è¿™ä¸ªç½‘ç«™ä¸Šè‡ªåŠ¨é¦–å­—æ¯å¤§å†™

- ç”»å›¾é‡‡ç”¨Ré‡Œæœ€æœ´ç´ çš„`plot`å’Œ`boxplot`å³å¯ï¼Œä½¿ç”¨ç°è‰²è‰²è°ƒï¼Œå¦‚æœä½¿ç”¨èŠ±é‡Œèƒ¡å“¨çš„é¢œè‰²å¯èƒ½ä¼šåœ¨äº¤ç‰ˆé¢è´¹çš„æ—¶å€™å¤šäº¤ğŸ’°500ğŸ˜­

# Reply Letter

### Point-By-Point Response to Reviewer

**ğŸš©å¥é¦–æ„Ÿè°¢Revieweræå‡ºå®è´µæ„è§**

- Thank you so much for this important comment.
- Thank you so much for the (highly) considerate advice.
- Thank you so much for the careful reading.


**ğŸš©å¥å°¾æ¬¢è¿Reviewerç»§ç»­æå‡ºå®è´µæ„è§**
- Your further comments would be much appreciated.
- Your further suggestions are very welcome.
